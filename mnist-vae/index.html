<html>
  <body>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px;">
        <h1 style="text-align: center;">Interactive Variational Autoencoder</h1>
        <p>
          Move your mouse over the graph to generate a digit.
        </p>
        <p>
          How it works: I trained a
          <a
            href="https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)"
            >variational autoencoder</a
          >
          (VAE) on the
          <a href="https://pytorch.org/vision/stable/datasets.html#mnist"
            >MNIST dataset</a
          >
          using a 2-dimensional latent space. The graph below shows each image
          in the training set encoded into its mean in the latent space, shown
          as an xy point, and colored with its respective label. I
          <a
            href="https://colab.research.google.com/drive/1wMTCIPeyim3r6DHPC2jDs2pbur7-WHOO?usp=sharing"
            >trained the model in Google Colab</a
          >
          with <a href="https://pytorch.org/">pytorch</a>, and exported the
          decoder to
          <a href="https://pytorch.org/docs/stable/onnx.html">ONNX format</a>,
          then used <a href="https://github.com/Microsoft/onnxjs">ONNX.js</a> to
          load the model and run it in the browser.
        </p>
        <p>
          The point that you move the mouse over is interpreted as a sample from
          the latent space. It is passed into the decoder to get an image, which
          is then drawn on a canvas to the right.
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <div>
        <img
          style="margin-right: 80px;"
          id="latent_space1"
          src="./latent_space.png"
        />
        <canvas id="canvas1" width="260" height="260"></canvas>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px; margin-top: 50px;">
        <h2 style="text-align: center;">
          Learning general characteristics
        </h2>
        <p>
          While exploring the digit clusters in the latent space above, I
          noticed that they each capture similar characteristics of a particular
          digit, like rotation and scaling. I was curious if the latent
          distribution could learn to capture these characteristics in a
          <i>general</i>
          way across all digits.
        </p>
        <p>
          I
          <a
            href="https://colab.research.google.com/drive/1XrAvC9ADIQtZ-J4Mc_i7unJUMbhi2-tx#scrollTo=dXokkZsZAj_k"
            >tweaked the autoencoder</a
          >
          by appending a 10-dimensional onehot vector representing the digit
          class to the 2 sampled latents before passing them into the decoder.
          My theory was that by explicitly providing the label to the decoder,
          the VAE would use the latent space to represent characterists that
          were label-invariant.
        </p>
        <p>
          As shown below, this caused the VAE's latent space to become a mess of
          color, indicating that the digits are evenly distributed throughout,
          and that the VAE likely learned to rely on the provided label rather
          than attempting to cluster digits. In fact, this model was able to
          achieve a much better loss than the previous one. And it indeed used
          the latents to represent general characteristics like shearing
          (x-axis) and boldness (y-axis).
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <img
        style="margin-right: 80px;"
        id="latent_space2"
        src="./latent_space_with_labels.png"
      />
    </div>
    <div
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        margin-top: 20px;
      "
    >
      <div>
        <canvas id="canvasLWL0" width="150" height="150"></canvas>
        <canvas id="canvasLWL1" width="150" height="150"></canvas>
        <canvas id="canvasLWL2" width="150" height="150"></canvas>
        <canvas id="canvasLWL3" width="150" height="150"></canvas>
        <canvas id="canvasLWL4" width="150" height="150"></canvas>
      </div>
      <div>
        <canvas id="canvasLWL5" width="150" height="150"></canvas>
        <canvas id="canvasLWL6" width="150" height="150"></canvas>
        <canvas id="canvasLWL7" width="150" height="150"></canvas>
        <canvas id="canvasLWL8" width="150" height="150"></canvas>
        <canvas id="canvasLWL9" width="150" height="150"></canvas>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px; margin-top: 50px;">
        <h1 style="text-align: center;">Controlling the latent space</h1>
        <p>
          VAEs are distinct from regular autoencoders due to the fact that their
          latent representation is a probability distribution, not an arbitrary
          vector. As a result, the VAE loss function has two terms: the
          reconstruction loss (which is the same as a regular autoencoder:
          comparing the input to the output) and a term measuring the difference
          between the current and desired latent probability distribution,
          computed with
          <a
            href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"
            >KL divergence</a
          >.
        </p>
        <p>
          Typically, and in the experiments above, the desired latent
          distribution is just a standard gaussian with means being 0 and
          variance being a diagonal matrix with all 1s. This causes the latents
          to be pushed around the origin, with similar data being clustered
          together (as seen above) making it convenient to sample, and yielding
          realistic interpolations between samples. Using a gaussian also makes
          it possible to compute the KL divergence loss term in closed form.
        </p>
        <p>
          I experimented with manipulating the latent distribution using the
          digit <i>labels</i>. Instead of computing the KL divergence loss with
          the same standard gaussian for each training image,
          <a
            href="https://colab.research.google.com/drive/1HxDSNIHEZ_c1y5imuhyeeTddIwxzm3Y-?usp=sharing"
            >I made the target means be different for each digit</a
          >
          (which required rederiving the closed form KL divergence loss term to
          have an arbitrary target mean). As shown below, this worked
          suprisingly well: the distributions of each digit were successfully
          forced around distinct, predefined points in the latent space. Even
          more surprising is the fact that interpolation between digits is still
          fairly smooth.
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <div>
        <img
          style="margin-right: 80px;"
          id="latent_space3"
          src="./latent_space_controlled.png"
        />
        <canvas id="canvas3" width="260" height="260"></canvas>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px; margin-top: 50px;">
        <h1 style="text-align: center;">Classifying the latent space</h1>
        <p>
          In the experiment above, prescribing means for each digit distribution
          caused distinct digit clusters. Trying to have clearly distinguishable
          digit clusters is effectively a classification task. To see if this
          goal could be made more explicit, I experimented with training the
          decoder to
          <i>jointly classify digits</i> alongside outputting image samples.
        </p>
        <p>
          <a
            href="https://colab.research.google.com/drive/1-jWyDoGDoiepaRSWiBMvl_Y_TP-rE_1N?usp=sharing"
            >In this experiement</a
          >, the decoder outputs both a sample image as well as a 10-element
          vector for digit classification. Each iteration of the training loop,
          the loss is computed by adding the VAE loss terms to a weighted
          classification loss. I had to tweak the weighting a bit before
          noticing an affect.
        </p>
        <p>
          The classification loss causes the encoder to separate digits in the
          latent space because the decoder must rely entirely on latent samples
          to classify the digits. The result are digit clusters with much
          cleaner boundaries than the vanilla VAE, while still being distributed
          around the origin. However, another effect I've observed is that
          interpolation becomes somewhat less realistic, since there's unused
          gaps between digit clusters. The size of the gaps depends on the
          weight of the classification loss term.
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <div style="display: flex;">
        <img
          style="margin-right: 80px;"
          id="latent_space4"
          src="./latent_space_classifier.png"
        />
        <canvas
          id="canvas4"
          width="260"
          height="260"
          style="margin-right: 80px;"
        ></canvas>
        <div>
          <h3 style="text-align: center;">Classifier output</h3>
          <canvas id="canvas5" width="260" height="200"></canvas>
        </div>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px; margin-top: 50px;">
        The classifier decision boundaries on the latent space are interesting
        to compare with the means of the training data. It appears this model
        had trouble distiguishing 4s and 9s.
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <div>
        <img src="./latent_space_boundaries.png" />
      </div>
    </div>
    <script src="onnx.min.js"></script>
    <script>
      class DigitCanvas {
        constructor(id) {
          const canvas = document.getElementById(id);
          const ctx = canvas.getContext('2d');
          const {width, height} = canvas;
          const [winc, hinc] = [width / 28, height / 28];
          this.props = {ctx, width, height, winc, hinc};
        }

        write(data) {
          const {ctx, width, height, winc, hinc} = this.props;
          ctx.clearRect(0, 0, width, height);
          for (let r = 0; r < 28; r++) {
            for (let c = 0; c < 28; c++) {
              const color = Math.floor(255 * data[r * 28 + c]);
              ctx.fillStyle = `rgb(${color}, ${color}, ${color})`;
              ctx.fillRect(winc * c, hinc * r, winc, hinc);
            }
          }
        }
      }

      class ClassCanvas {
        constructor(id) {
          const canvas = document.getElementById(id);
          const ctx = canvas.getContext('2d');
          const {width, height} = canvas;
          const hinc = height / 10;
          this.props = {ctx, width, height, hinc};
        }

        write(data) {
          const {ctx, width, height, hinc} = this.props;
          ctx.clearRect(0, 0, width, height);
          ctx.strokeStyle = 'gray';
          for (let r = 0; r < 10; r++) {
            const p = Math.round(data[r] * 1000000) / 1000000;
            ctx.fillText(r, 0, hinc * (r + 0.5));
            ctx.fillRect(20, hinc * r, width * p, hinc);
            ctx.strokeText(p, 20, hinc * (r + 0.5));
          }
        }
      }

      async function predict(session, x, y) {
        const tensorInputs = [
          new onnx.Tensor(new Float32Array([x, y]), 'float32', [1, 2]),
        ];
        const output = await session.run(tensorInputs);
        const outputTensor = output.values().next().value;
        return outputTensor.data;
      }

      async function predictLWL(session, x, y) {
        const latents = [];
        const emptyClass = Array(10).fill(0);
        for (let i = 0; i < 10; i++) {
          const onehotClass = [...emptyClass];
          onehotClass[i] = 1;
          latents.push(...[x, y, ...onehotClass]);
        }
        const tensorInputs = [
          new onnx.Tensor(new Float32Array(latents), 'float32', [10, 12]),
        ];
        const output = await session.run(tensorInputs);
        const outputTensor = output.values().next().value;
        return outputTensor.data;
      }

      async function predictClassifier(session, x, y) {
        const tensorInputs = [
          new onnx.Tensor(new Float32Array([x, y]), 'float32', [1, 2]),
        ];
        const output = await session.run(tensorInputs);
        const outputTensor = output.values().next().value;
        return outputTensor.data;
      }

      // 2d vae
      async function initVAE() {
        const vaeMnist = new onnx.InferenceSession();
        await vaeMnist.loadModel(window.location.href + '/vae_mnist.onnx');

        const canvas = new DigitCanvas('canvas1');
        const data = await predict(vaeMnist, 0, 0);
        canvas.write(data);

        const latentSpace = document.getElementById('latent_space1');
        latentSpace.onmousemove = async e => {
          const {offsetX, offsetY} = e;
          const [x, y] = [
            ((offsetX - 175) / 120) * 3.3,
            (-(offsetY - 142) / 97) * 4.2,
          ];
          const data = await predict(vaeMnist, x, y);
          canvas.write(data);
        };
      }

      // 2d vae using latents with class labels (LWL)
      async function initVAELWL() {
        const vaeMnistLWL = new onnx.InferenceSession();
        await vaeMnistLWL.loadModel(
          window.location.href + '/vae_mnist_latent_with_labels.onnx',
        );

        const canvases = Array(10)
          .fill()
          .map((_, i) => new DigitCanvas('canvasLWL' + i));
        const data = await predictLWL(vaeMnistLWL, 0, 0);
        const s = 28 * 28;
        for (let i = 0; i < 10; i++) {
          const img = data.slice(s * i, s * (i + 1));
          canvases[i].write(img);
        }

        const latentSpace = document.getElementById('latent_space2');
        let pending = false;
        latentSpace.onmousemove = async e => {
          if (pending) {
            return;
          }
          const {offsetX, offsetY} = e;
          const [x, y] = [
            ((offsetX - 175) / 120) * 4.0,
            (-(offsetY - 130) / 97) * 4.0,
          ];
          pending = true;
          const data = await predictLWL(vaeMnistLWL, x, y);
          pending = false;
          for (let i = 0; i < 10; i++) {
            const img = data.slice(s * i, s * (i + 1));
            canvases[i].write(img);
          }
        };
      }

      // 2d vae using latents with class labels (LWL)
      async function initVAEControlled() {
        const vaeMnist = new onnx.InferenceSession();
        await vaeMnist.loadModel(
          window.location.href + '/vae_mnist_controlled.onnx',
        );

        const canvas = new DigitCanvas('canvas3');
        const data = await predict(vaeMnist, 0, 0);
        canvas.write(data);

        const latentSpace = document.getElementById('latent_space3');
        latentSpace.onmousemove = async e => {
          const {offsetX, offsetY} = e;
          const [x, y] = [(offsetX - 171) / 7.8, -(offsetY - 116) / 4.2];
          const data = await predict(vaeMnist, x, y);
          canvas.write(data);
        };
      }

      async function initVAEClassifier() {
        const vaeMnist = new onnx.InferenceSession();
        await vaeMnist.loadModel(
          window.location.href + '/vae_mnist_classifier.onnx',
        );

        const canvas = new DigitCanvas('canvas4');
        const classCanvas = new ClassCanvas('canvas5');
        const data = await predict(vaeMnist, 0, 0);
        canvas.write(data);
        classCanvas.write(data.slice(-10));

        const latentSpace = document.getElementById('latent_space4');
        latentSpace.onmousemove = async e => {
          const {offsetX, offsetY} = e;
          const [x, y] = [
            ((offsetX - 164) / 100) * 4,
            (-(offsetY - 147) / 95) * 4,
          ];
          const data = await predictClassifier(vaeMnist, x, y);
          canvas.write(data);
          classCanvas.write(data.slice(-10));
        };
      }

      async function main() {
        await initVAE();
        await initVAELWL();
        await initVAEControlled();
        await initVAEClassifier();
      }

      main();
    </script>
  </body>
</html>
