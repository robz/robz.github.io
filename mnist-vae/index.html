<html>
  <body>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px;">
        <h1 style="text-align: center;">Interactive Variational Autoencoder</h1>
        <p>
          Move your mouse over the graph to generate a digit.
        </p>
        <p>
          How it works: I trained a
          <a
            href="https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)"
            >variational autoencoder</a
          >
          (VAE) on the
          <a href="https://pytorch.org/vision/stable/datasets.html#mnist"
            >MNIST dataset</a
          >
          using a 2-dimensional latent space. The graph below shows each image
          in the training set encoded into its mean in the latent space, shown
          as an xy point, and colored with its respective label. I
          <a
            href="https://colab.research.google.com/drive/1wMTCIPeyim3r6DHPC2jDs2pbur7-WHOO?usp=sharing"
            >trained the model in Google Colab</a
          >
          with <a href="https://pytorch.org/">pytorch</a>, and exported the
          decoder to
          <a href="https://pytorch.org/docs/stable/onnx.html">ONNX format</a>,
          then used <a href="https://github.com/Microsoft/onnxjs">ONNX.js</a> to
          load the model and run it in the browser.
        </p>
        <p>
          The point that you move the mouse over is interpreted as a sample from
          the latent space. It is passed into the decoder to get an image, which
          is then drawn on a canvas to the right.
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <div>
        <img
          style="margin-right: 80px;"
          id="latent_space1"
          src="./latent_space.png"
        />
        <canvas id="canvas1" width="260" height="260"></canvas>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px; margin-top: 50px;">
        <h2 style="text-align: center;">
          Learning general characteristics
        </h2>
        <p>
          While exploring the digit clusters in the latent space above, I
          noticed that they each capture similar characteristics of a particular
          digit, like rotation and scaling. I was curious if the latent
          distribution could learn to capture these characteristics in a general
          way across all digits.
        </p>
        <p>
          I
          <a
            href="https://colab.research.google.com/drive/1XrAvC9ADIQtZ-J4Mc_i7unJUMbhi2-tx#scrollTo=dXokkZsZAj_k"
            >tweaked the autoencoder</a
          >
          by appending a 10-dimensional onehot vector representing the digit
          class to the 2 sampled latents before passing them into the decoder.
          My theory was that by explicitly providing the label to the decoder,
          the VAE would use the latent space to represent characterists that
          were label-invariant.
        </p>
        <p>
          As shown below, this caused the VAE's latent space to become a mess of
          color, indicating that the digits are evenly distributed throughout,
          and that the VAE likely learned to rely on the provided label rather
          than attempting to cluster digits. In fact, this model was able to
          achieve a much better loss than the previous one. And it indeed used
          the latents to represent general characteristics like shearing
          (x-axis) and boldness (y-axis).
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <img
        style="margin-right: 80px;"
        id="latent_space2"
        src="./latent_space_with_labels.png"
      />
    </div>
    <div
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        margin-top: 20px;
      "
    >
      <div>
        <canvas id="canvasLWL0" width="150" height="150"></canvas>
        <canvas id="canvasLWL1" width="150" height="150"></canvas>
        <canvas id="canvasLWL2" width="150" height="150"></canvas>
        <canvas id="canvasLWL3" width="150" height="150"></canvas>
        <canvas id="canvasLWL4" width="150" height="150"></canvas>
      </div>
      <div>
        <canvas id="canvasLWL5" width="150" height="150"></canvas>
        <canvas id="canvasLWL6" width="150" height="150"></canvas>
        <canvas id="canvasLWL7" width="150" height="150"></canvas>
        <canvas id="canvasLWL8" width="150" height="150"></canvas>
        <canvas id="canvasLWL9" width="150" height="150"></canvas>
      </div>
    </div>
    <div style="display: flex; justify-content: center;">
      <section style="width: 700px; margin-top: 50px;">
        <h1 style="text-align: center;">Controlling the latent space</h1>
        <p>
          VAEs are distinct from regular autoencoders due to the fact that their
          latent representation is a probability distribution, not an arbitrary
          vector. As a result, the VAE loss function has two terms: the
          reconstruction loss (which is the same as a regular autoencoder,
          comparing the input to the output) and a term measuring the difference
          between the current and desired latent probability distribution,
          computed with
          <a
            href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"
            >KL divergence</a
          >.
        </p>
        <p>
          Typically, the desired latent distribution is just a standard gaussian
          with mean 0 and sigma being a diagonal matrix with all 1s. This causes
          the latents to be pushed near to the origin, with similar data being
          clustered together (as seen above) making it convenient to sample, and
          yielding realistic interpolations between samples. Choosing a gaussian
          also makes it possible to compute the KL divergence loss term in
          closed form.
        </p>
        <p>
          I experimented with manipulating the latent distributions using the
          digit labels. Instead of computing the KL divergence loss based on a
          standard gaussian,
          <a
            href="https://colab.research.google.com/drive/1HxDSNIHEZ_c1y5imuhyeeTddIwxzm3Y-?usp=sharing"
            >I made the mean of the target gaussian be different for each
            digit</a
          >
          (which required rederiving the closed form KL divergence loss term).
          As shown below, this works suprisingly well: the distributions of each
          digit were successfully forced into distinct areas of the latent
          space. Even more surprising is the fact that interpolation between
          digits is still smooth.
        </p>
      </section>
    </div>
    <div style="display: flex; justify-content: center; margin-top: 20px;">
      <div>
        <img
          style="margin-right: 80px;"
          id="latent_space3"
          src="./latent_space_controlled.png"
        />
        <canvas id="canvas3" width="260" height="260"></canvas>
      </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"></script>
    <script>
      class DigitCanvas {
        constructor(id) {
          const canvas = document.getElementById(id);
          const ctx = canvas.getContext('2d');
          const {width, height} = canvas;
          const [winc, hinc] = [width / 28, height / 28];
          this.props = {ctx, width, height, winc, hinc};
        }

        write(data) {
          const {ctx, width, height, winc, hinc} = this.props;
          ctx.clearRect(0, 0, width, height);
          for (let r = 0; r < 28; r++) {
            for (let c = 0; c < 28; c++) {
              const color = Math.floor(255 * data[r * 28 + c]);
              ctx.fillStyle = `rgb(${color}, ${color}, ${color})`;
              ctx.fillRect(winc * c, hinc * r, winc, hinc);
            }
          }
        }
      }

      async function predict(session, x, y) {
        const tensorInputs = [
          new onnx.Tensor(new Float32Array([x, y]), 'float32', [1, 2]),
        ];
        const output = await session.run(tensorInputs);
        const outputTensor = output.values().next().value;
        return outputTensor.data;
      }

      async function predictLWL(session, x, y) {
        const latents = [];
        const emptyClass = Array(10).fill(0);
        for (let i = 0; i < 10; i++) {
          const onehotClass = [...emptyClass];
          onehotClass[i] = 1;
          latents.push(...[x, y, ...onehotClass]);
        }
        const tensorInputs = [
          new onnx.Tensor(new Float32Array(latents), 'float32', [10, 12]),
        ];
        const output = await session.run(tensorInputs);
        const outputTensor = output.values().next().value;
        return outputTensor.data;
      }

      // 2d vae
      async function initVAE() {
        const vaeMnist = new onnx.InferenceSession();
        await vaeMnist.loadModel(window.location.href + '/vae_mnist.onnx');

        const canvas = new DigitCanvas('canvas1');
        const data = await predict(vaeMnist, 0, 0);
        canvas.write(data);

        const latentSpace = document.getElementById('latent_space1');
        latentSpace.onmousemove = async e => {
          const {offsetX, offsetY} = e;
          const [x, y] = [
            ((offsetX - 175) / 120) * 3.3,
            (-(offsetY - 142) / 97) * 4.2,
          ];
          const data = await predict(vaeMnist, x, y);
          canvas.write(data);
        };
      }

      // 2d vae using latents with class labels (LWL)
      async function initVAELWL() {
        const vaeMnistLWL = new onnx.InferenceSession();
        await vaeMnistLWL.loadModel(
          window.location.href + '/vae_mnist_latent_with_labels.onnx',
        );

        const canvases = Array(10)
          .fill()
          .map((_, i) => new DigitCanvas('canvasLWL' + i));
        const data = await predictLWL(vaeMnistLWL, 0, 0);
        const s = 28 * 28;
        for (let i = 0; i < 10; i++) {
          const img = data.slice(s * i, s * (i + 1));
          canvases[i].write(img);
        }

        const latentSpace = document.getElementById('latent_space2');
        let pending = false;
        latentSpace.onmousemove = async e => {
          if (pending) {
            return;
          }
          const {offsetX, offsetY} = e;
          const [x, y] = [
            ((offsetX - 175) / 120) * 4.0,
            (-(offsetY - 130) / 97) * 4.0,
          ];
          console.log(x, y);
          pending = true;
          const data = await predictLWL(vaeMnistLWL, x, y);
          pending = false;
          for (let i = 0; i < 10; i++) {
            const img = data.slice(s * i, s * (i + 1));
            canvases[i].write(img);
          }
        };
      }

      // 2d vae using latents with class labels (LWL)
      async function initVAEControlled() {
        const vaeMnist = new onnx.InferenceSession();
        await vaeMnist.loadModel(
          window.location.href + '/vae_mnist_controlled.onnx',
        );

        const canvas = new DigitCanvas('canvas3');
        const data = await predict(vaeMnist, 0, 0);
        canvas.write(data);

        const latentSpace = document.getElementById('latent_space3');
        latentSpace.onmousemove = async e => {
          const {offsetX, offsetY} = e;
          const [x, y] = [(offsetX - 171) / 7.8, -(offsetY - 116) / 4.2];
          console.log(offsetX, offsetY, x, y);
          const data = await predict(vaeMnist, x, y);
          canvas.write(data);
        };
      }

      async function main() {
        await initVAE();
        await initVAELWL();
        await initVAEControlled();
      }

      main();
    </script>
  </body>
</html>
